{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arm', 'boy', 'bread', 'chicken', 'child', 'computer', 'ear', 'house', 'leg', 'sandwich', 'television', 'truck', 'vehicle', 'watch', 'woman']\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as utils\n",
    "import torch.optim as optim      # implementing various optimization algorithms\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "import gzip\n",
    "import _pickle as cPickle\n",
    "import os\n",
    "from collections import Counter\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "WORD2VECPATH    = \"data/class_vectors.npy\"\n",
    "DATAPATH        = \"data/zeroshot_data.pkl\"\n",
    "MODELPATH       = \"model/\"\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "NUM_CLASS = 15\n",
    "NUM_ATTR = 300\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 65\n",
    "with open('txtfile/train_classes.txt', 'r') as infile:\n",
    "    train_classes = [str.strip(line) for line in infile]\n",
    "print(train_classes)\n",
    "with open('txtfile/zsl_classes.txt', 'r') as infile:\n",
    "    zsl_classes = [str.strip(line) for line in infile]\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"read data, create datasets\"\"\"\n",
    "    # READ DATA\n",
    "    with gzip.GzipFile(DATAPATH, 'rb') as infile:\n",
    "        data = cPickle.load(infile)\n",
    "\n",
    "    # ONE-HOT-ENCODE DATA\n",
    "    label_encoder   = LabelEncoder()\n",
    "    label_encoder.fit(train_classes)\n",
    "\n",
    "    training_data = [instance for instance in data if instance[0] in train_classes]\n",
    "    zero_shot_data = [instance for instance in data if instance[0] not in train_classes]\n",
    "    # SHUFFLE TRAINING DATA\n",
    "    np.random.shuffle(training_data)\n",
    "\n",
    "    ### SPLIT DATA FOR TRAINING\n",
    "    train_size  = 300\n",
    "    train_data  = list()\n",
    "    valid_data  = list()\n",
    "    for class_label in train_classes:\n",
    "        ct = 0\n",
    "        for instance in training_data:\n",
    "            if instance[0] == class_label:\n",
    "                if ct < train_size:\n",
    "                    train_data.append(instance)\n",
    "                    ct+=1\n",
    "                    continue\n",
    "                valid_data.append(instance)\n",
    "\n",
    "    # SHUFFLE TRAINING AND VALIDATION DATA\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.shuffle(valid_data)\n",
    "\n",
    "    train_data = [(instance[1], to_categorical(label_encoder.transform([instance[0]]), num_classes=15))for instance in train_data]\n",
    "    valid_data = [(instance[1], to_categorical(label_encoder.transform([instance[0]]), num_classes=15)) for instance in valid_data]\n",
    "\n",
    "    # FORM X_TRAIN AND Y_TRAIN\n",
    "    x_train, y_train    = zip(*train_data)\n",
    "    x_train, y_train    = np.squeeze(np.asarray(x_train)), np.squeeze(np.asarray(y_train))\n",
    "    # L2 NORMALIZE X_TRAIN\n",
    "    x_train = normalize(x_train, norm='l2')\n",
    "\n",
    "    # FORM X_VALID AND Y_VALID\n",
    "    x_valid, y_valid = zip(*valid_data)\n",
    "    x_valid, y_valid = np.squeeze(np.asarray(x_valid)), np.squeeze(np.asarray(y_valid))\n",
    "    # L2 NORMALIZE X_VALID\n",
    "    x_valid = normalize(x_valid, norm='l2')\n",
    "\n",
    "\n",
    "    # FORM X_ZSL AND Y_ZSL\n",
    "    y_zsl, x_zsl = zip(*zero_shot_data)\n",
    "    x_zsl, y_zsl = np.squeeze(np.asarray(x_zsl)), np.squeeze(np.asarray(y_zsl))\n",
    "    # L2 NORMALIZE X_ZSL\n",
    "    x_zsl = normalize(x_zsl, norm='l2')\n",
    "\n",
    "    print(\"-> data loading is completed.\")\n",
    "    tensor_x = torch.stack([torch.Tensor(i) for i in x_train]) # transform to torch tensors\n",
    "    tensor_y = torch.stack([torch.Tensor(i) for i in y_train])\n",
    "    \n",
    "    my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    my_dataloader = utils.DataLoader(my_dataset) # create your dataloader\n",
    "    trainset_loader = DataLoader(my_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "    tensor_x = torch.stack([torch.Tensor(i) for i in x_valid]) # transform to torch tensors\n",
    "    tensor_y = torch.stack([torch.Tensor(i) for i in y_valid])\n",
    "\n",
    "    my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    my_dataloader = utils.DataLoader(my_dataset) # create your dataloader\n",
    "    validset_loader = DataLoader(my_dataset, batch_size=40, shuffle=True, num_workers=0)\n",
    "    \n",
    "#     tensor_x = torch.stack([torch.Tensor(i) for i in x_zsl]) # transform to torch tensors\n",
    "#     tensor_y = torch.stack([torch.Tensor(i) for i in y_zsl])\n",
    "\n",
    "#     my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "#     my_dataloader = utils.DataLoader(my_dataset) # create your dataloader\n",
    "#     testzsl_loader = DataLoader(my_dataset, batch_size=40, shuffle=True, num_workers=0)\n",
    "    \n",
    "    return trainset_loader, validset_loader, x_zsl, y_zsl\n",
    "#     return (x_train, x_valid, x_zsl), (y_train, y_valid, y_zsl)\n",
    "\n",
    "def get_features(image_name):\n",
    "    \n",
    "    img = Image.open(image_name)\n",
    "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n",
    "    print(t_img.shape)\n",
    "    vgg16 = models.vgg16(pretrained=True)\n",
    "    new_base =  (list(vgg16.children())[:-1])[0]\n",
    "    output = vgg16.features[-1](t_img)\n",
    "    print(output.shape)\n",
    "    return output\n",
    "\n",
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizerâ€™s states and hyperparameters used.\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)\n",
    "    \n",
    "    \n",
    "def valid(model,validset_loader,device,validlis,losslis_val,reshape=False):\n",
    "    model.eval()  # set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validset_loader:\n",
    "            if reshape==False:\n",
    "                target=target.argmax(1).type(torch.long)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data=data.float()\n",
    "            target=target.type(torch.long)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(validset_loader.dataset)\n",
    "    losslis_val.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(validset_loader.dataset),\n",
    "        100. * correct / len(validset_loader.dataset)))   \n",
    "    validlis.append(100. * correct / len(validset_loader.dataset))\n",
    "\n",
    "def train_save(model,epoch, save_interval, log_interval,trainset_loader,validset_loader,device,optimizer,losslis,acclis,validlis,losslis_val,reshape=False):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "\n",
    "    for ep in range(epoch):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            if reshape==False:\n",
    "                target=target.argmax(1).type(torch.long)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data=data.float()\n",
    "            target=target.type(torch.long)\n",
    "#             print(data.shape)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # print(output.shape,target.shape)\n",
    "             \n",
    "            loss = criterion(output, target)\n",
    "            train_loss+=loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "#             acc=np.sum(np.equal(output,target))/target.shape[0]\n",
    "            \n",
    "#             total_acc+=acc\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            # different from before: saving model checkpoints\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('nist36-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        train_loss /= len(trainset_loader.dataset)*40\n",
    "#         total_acc/=len(batches)\n",
    "\n",
    "        losslis.append(train_loss)\n",
    "        acclis.append(100.* correct /len(trainset_loader.dataset))\n",
    "        valid(model,validset_loader,device,validlis,losslis_val,reshape)\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('nist36-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Linear(in_features, out_features, bias=True)\n",
    "        self.fc1 = nn.Linear(4096, 800)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout2d(0.5)\n",
    "        self.fc2 = nn.Linear(800, 500)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout2d(0.5)\n",
    "        self.fc3 = nn.Linear(500, NUM_ATTR)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(NUM_ATTR,NUM_CLASS)\n",
    "        self.softmax=nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "#         print(x.shape)\n",
    "        x=self.drop1(x)\n",
    "#         print(x.shape)\n",
    "        x=self.relu2(self.fc2(x))\n",
    "#         print(x.shape)\n",
    "        x=self.drop2(x)\n",
    "#         print(x.shape)\n",
    "        x = self.relu3(self.fc3(x))\n",
    "#         print(x.shape)\n",
    "#         print(self.fc4(x).shape)\n",
    "        x = self.softmax(self.fc4(x))\n",
    "#         print(x)\n",
    "        return x\n",
    "def custom_kernel_init(shape,dtype=None):\n",
    "    class_vectors       = np.load(WORD2VECPATH,allow_pickle=True)\n",
    "    training_vectors    = sorted([(label, vec) for (label, vec) in class_vectors if label in train_classes], key=lambda x: x[0])\n",
    "    classnames, vectors = zip(*training_vectors)\n",
    "    vectors             = np.asarray(vectors, dtype=np.float)\n",
    "    vectors             = vectors.T\n",
    "    return vectors\n",
    "\n",
    "def train_mynet(trainset_loader,testset_loader):\n",
    "    maxepoch=10\n",
    "    losslis=[]\n",
    "    acclis=[]\n",
    "    validlis=[]\n",
    "    losslis_val=[]\n",
    "    epochs=np.arange(maxepoch)\n",
    "    save_interval, log_interval=500,100\n",
    "\n",
    "    mymodel = MyNet().to(device)\n",
    "    numpy_data=custom_kernel_init((300,15),np.float64)\n",
    "#     print(\"numpoy shape\",numpy_data.shape)\n",
    "\n",
    "    mymodel.fc4.weight.requires_grad = False\n",
    "    mymodel.fc4.weight.data._copy=torch.FloatTensor(numpy_data)\n",
    "\n",
    "    \n",
    "    print(modulelist)\n",
    "    for l in modulelist[:5]:\n",
    "        x = l(x)\n",
    "        keep = x\n",
    "    print(mymodel.fc3)\n",
    "    layers = [x for x in mymodel.parameters()]\n",
    "    for l in layers:\n",
    "        print(l.shape,l.requires_grad)\n",
    "    optimizer = optim.Adam(mymodel.parameters(), lr=0.001)\n",
    "#     train_save(mymodel,maxepoch, save_interval, log_interval,trainset_loader,testset_loader,device,optimizer,losslis,acclis,validlis,losslis_val,False)\n",
    "#     print(losslis)\n",
    "#     print(acclis)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> data loading is completed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'numpy.str_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-041cd50f303b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidset_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_zsl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_zsl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     train_mynet(trainset_loader,validset_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#zsl model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-80e7a44b9578>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mtensor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_zsl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# transform to torch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_zsl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmy_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create your datset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-80e7a44b9578>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mtensor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_zsl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# transform to torch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_zsl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmy_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create your datset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'numpy.str_'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainset_loader, validset_loader,x_zsl,y_zsl=load_data()\n",
    "#     train_mynet(trainset_loader,validset_loader)\n",
    "    #zsl model\n",
    "    mymodel = MyNet().to(device)\n",
    "    optimizer = optim.Adam(mymodel.parameters(), lr=0.001)\n",
    "    checkpoint=load_checkpoint('nist36-710.pth', mymodel, optimizer)\n",
    "    modulelist = list(mymodel.modules())\n",
    "    model = nn.Sequential(*modulelist[1:-2])\n",
    "    print(model)\n",
    "    \n",
    "    output = model(x_zsl)\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    \n",
    "#     pred_zsl    = zsl_model.predict(x_zsl)\n",
    "\n",
    "    top5, top3, top1 = 0, 0, 0\n",
    "    for i, pred in enumerate(pred_zsl):\n",
    "        pred            = np.expand_dims(pred, axis=0)\n",
    "        dist_5, index_5 = tree.query(pred, k=5)\n",
    "        pred_labels     = [classnames[index] for index in index_5[0]]\n",
    "        true_label      = y_zsl[i]\n",
    "        if true_label in pred_labels:\n",
    "            top5 += 1\n",
    "        if true_label in pred_labels[:3]:\n",
    "            top3 += 1\n",
    "        if true_label in pred_labels[0]:\n",
    "            top1 += 1\n",
    "\n",
    "    print()\n",
    "    print(\"ZERO SHOT LEARNING SCORE\")\n",
    "    print(\"-> Top-5 Accuracy: %.2f\" % (top5 / float(len(x_zsl))))\n",
    "    print(\"-> Top-3 Accuracy: %.2f\" % (top3 / float(len(x_zsl))))\n",
    "    print(\"-> Top-1 Accuracy: %.2f\" % (top1 / float(len(x_zsl))))\n",
    "    \n",
    "    \n",
    "    valid(model,zsl_loader,device,validlis,losslis_val,False)\n",
    "#     print(modulelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_zsl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vectors       = sorted(np.load(WORD2VECPATH,allow_pickle=True), key=lambda x: x[0])\n",
    "classnames, vectors = zip(*class_vectors)\n",
    "classnames          = list(classnames)\n",
    "vectors             = np.asarray(vectors, dtype=np.float)\n",
    "\n",
    "tree        = KDTree(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (fc1): Linear(in_features=4096, out_features=800, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=500, out_features=300, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc4): Linear(in_features=300, out_features=15, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "torch.Size([800, 4096]) True\n",
      "torch.Size([800]) True\n",
      "torch.Size([500, 800]) True\n",
      "torch.Size([500]) True\n",
      "torch.Size([300, 500]) True\n",
      "torch.Size([300]) True\n",
      "torch.Size([15, 300]) True\n",
      "torch.Size([15]) True\n"
     ]
    }
   ],
   "source": [
    "# mymodel = MyNet().to(device)\n",
    "\n",
    "print(mymodel)\n",
    "layers = [x for x in mymodel.parameters()]\n",
    "for l in layers:\n",
    "    o=1\n",
    "    print(l.shape,l.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyNet().to(device)\n",
    "   \n",
    "#     mymodel.fc4.weight.requires_grad = False\n",
    "layers = [x.data for x in mymodel.parameters()]\n",
    "#     for l in layers:\n",
    "#         print(l.shape)\n",
    "#     model.g1[0].weight.data.copy_(vgg.features[0].weight.data)\n",
    "#     model.fc4.weight.data.copy_(vgg.features[0].weight.data)\n",
    "numpy_data=custom_kernel_init(15,np.float64)\n",
    "# print(numpy_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[7].data = torch.FloatTensor(numpy_data)\n",
    "\n",
    "mymodel.fc4.weight.requires_grad = False\n",
    "mymodel.fc4.weight.data=torch.FloatTensor(numpy_data)\n",
    "print(mymodel.fc4.weight.requires_grad)\n",
    "print(mymodel.fc4.weight)\n",
    "print(mymodel.fc4.parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
